{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff63b0b5",
   "metadata": {},
   "source": [
    "Give the token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6c21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbfd62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4446ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "#may have to change so find the best for performance and quality\n",
    "learning_rate = 3e-4 \n",
    "eval_iters = 250\n",
    "# disabled in eval mode, drops random neurons in the netwrk to prevent overfitting\n",
    "#dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b451204-4211-4882-abca-c62957d95f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size=len(chars)\n",
    "# print(chars)\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac62b",
   "metadata": {},
   "source": [
    "Backgroud infomration about tokenizers\n",
    "\n",
    "Right now a character level tokenizer is bs being used, which takes each character and coverts it into a integer equivalent \n",
    "we have a small vocabulary to work with but a lot  of characters to encode and decode.\n",
    "\n",
    "A word level tokenizer takes each word and conversts it into a intger equivalent. \n",
    "Has a a massive vocabulary but has a relatively small amount to encode and decode. \n",
    "\n",
    "A subword tokenizer is between a charcter and word level tokenizer in terms of amount to encode and decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66afddc0-77dd-4097-84b6-545332cf409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[ch] for ch in s]\n",
    "decode = lambda x: ''.join([int_to_string[i] for i in x])\n",
    "\n",
    "#basically a super long sequence of characters\n",
    "#kind of think of it as an array of characters\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e78ebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[65, 65,  1, 73, 61, 58, 72, 58],\n",
      "        [67, 11,  1, 32, 54, 57,  1, 73],\n",
      "        [68, 67, 58,  1, 69, 58, 68, 69],\n",
      "        [54, 69, 69, 58, 67,  9,  3,  1]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[65,  1, 73, 61, 58, 72, 58,  1],\n",
      "        [11,  1, 32, 54, 57,  1, 73, 61],\n",
      "        [67, 58,  1, 69, 58, 68, 69, 65],\n",
      "        [69, 69, 58, 67,  9,  3,  1, 72]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d5660",
   "metadata": {},
   "source": [
    "The 'estimate_loss' function is a utility that helps you understand how well your model is performing by calculating and returning the average loss on both the training and validation datasets. By running this function periodically during training, you can monitor whether your model is improving and whether it's potentially overfitting (performing well on training data but poorly on validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6355dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = loss.item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc22995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------Test-----------------###\n",
    "# x and y are sequences of length 'block_size' where y is the same as x but shifted one position to the right\n",
    "# This a setup for a sequence prediction task where the model has to predict the next character in a sequence\n",
    "# x=train_data[:block_size]\n",
    "# y=train_data[1:block_size+1]\n",
    "\n",
    "# for t in range(block_size):\n",
    "#     context = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print('when input is', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6487a",
   "metadata": {},
   "source": [
    "by having the nn.module as a parameter any torch.nn layers such as nn.Layers become learnable through gradient decesent. \n",
    "What is gradient decesent? Gradient desent is a optimizer for the language model, to make more accurate predictions. Before it can be further explained we must discuss the loss. Lets say we started the model with no training, with random weights. Then, we have about a 1 in 81 chance of accuratly predicting the next charater. Its a 1 in 81 chance becuase we are attempting to predict the next character (singular so 1 character) while we have a total of 81 chacter that this model will be trained on. In order to find the loss we must get negative log likely hood which is -ln(1/81) which is 4.39444915467. This is awful we want the loss to be as close as zero as possible inorder to increase the prediction accuracy or minumize the loss. How do we do this? We have to take the derivate of the loss at the current point and move it in a manner that decreases the slope or makes the slope in a negative direction, remember the slope is the derivative. So, if we notice, the slope or derivate becoming smaller we know we are going in the correct direction in optimization. This process is the gradient decesent, its the slow process of decreasing the loss which it increasing the optimization.\n",
    "\n",
    "In this project we will be using AdamW ( Found at https://pytorch.org/docs/stable/optim.html ), a optimization algorithm provided by pytorch. AdamW has an adaptive learning rate for different parameters which can help speed up the process of minimizing the loss as discussed above. We are using this becomes it has has weight decay which genralized the paremters more. This helps the model from memorizing the data too closerly instead of learing patterns. This helps improve the model's performance on new and unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f59f0236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rFCE7IHBzOKheAmqj(Gu2﻿a2OiYeCxxuFt&Fnw\n",
      "ghdMP4T\n",
      "bd;0N_jsZXH0\n",
      "(jXlVLdU.kW(D \"aj4&l DSzx\n",
      "EGlV9;﻿jhsU]!8wcEq2M\n",
      "XPB﻿&rE4.f__]g6dMb&[Dh1G]T:iGU:Uv)]fkV(VgGnkyZ2nDR99iSCQJIPN v2]0\n",
      "Pv6X eEVj43Nd84ECSNq,H8& KGIdVEgajf&d'SvY:;!!0ncZBHi﻿5b&eeMP,?Mu,AlzK4qo4yy:J' A5AW1L71gEYX8GabOCrBDamYXzssoFHFTr5MVRXQWC?,uHikU\"35qlP\"R,FrVenLDk7'p6pWImt?e4A4!f4)Wnhj.I_jl]m3;kU.Bq0KA*ML?7Ep3PneAQeeMbp0pCtl7!dxcUjko!6q-[-4G_[oQJi:I[!: eu,xV2]9JYWBkiKuzsX7'.bQajeLjw!_SF(]bdXUao!_xi﻿omY ETZLeAWDX)2lV(HY\n",
      ".cWz﻿6T R\n",
      "EoEZC:E00 K m\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__ (self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # logits can be thought of, as a buunch of flating point digists that have been normalized\n",
    "    # Nomralized is the contribution divided by the sum of everything\n",
    "    def forward(self, index, targets=None):\n",
    "        # probability distribution of what we want to predict\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # since the batch and time isnt as important so they can be blended together, as long as teh logits and targets have the smae batch and time\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)    \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B,T) array of indicies in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (B,C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # becomes (B,1)\n",
    "            index = torch.cat((index, index_next), dim = 1) # becomes (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6459ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstep: 0, train loss 4.8842, val loss 4.6981\n",
      "lstep: 250, train loss 4.5422, val loss 4.5333\n",
      "lstep: 500, train loss 4.6748, val loss 4.2083\n",
      "lstep: 750, train loss 4.3345, val loss 4.2135\n",
      "4.341006755828857\n"
     ]
    }
   ],
   "source": [
    "#create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"lstep: {iter}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    #sample a batch of data\n",
    "    xb,yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ecb8a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ";MP;c90'c9Jn oM[Zp]ywWgeB?(tCX yq]a5)\n",
      "JW?qv)H&xMhuH0NYWCG l p﻿ZPKiljHJl *Sy0q\"[﻿t?3 yNt.7k09K!!nsMR&SdMd :t\n",
      "F2OVA6w3P\n",
      "ElY_0g(cG5.Da\n",
      "Et2,pvp&Qx,*4zBX*7u[n( vLjgNq[MLk37NqzTT[KEVRrnUad?UaQAuH-\"PCQ7\n",
      "fG(N':u400\n",
      "I4Sz*mR\",C!Oi6X : Qe\n",
      "Z_(pp7561zzKjem,?X;yfr_-(hDPM:-Lc9Moa7EuO\n",
      "EqJ?3fhaOiOOQ\"RjNdT75J?7NPCQT)Id8!VD66Ntd-Y*7KH-'q0E[K&YRE6'qzhiaur_W-(H&Ui1r!Quy,eaeK2F8hT7Zd 3h,ivyieAZEX*b[10M4z﻿1﻿ZdCX8kplJuO3Fv'.tLB7? VX(mE u&JlYWz(w_P\"PLaJ7B:iOGCKubpWI'JfOKjX)O813!KQ﻿zX 0'uBD*M)Y_﻿TRnO\n",
      "ColK].5Ua3nofi!QtdMP\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76cfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
